{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2631"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os,gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "user_folder = os.path.expanduser(\"~\")\n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"), 'E:/git/database/Toxic_Comment')\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_target = pd.read_csv(os.path.join(data_folder, \"train.csv\"))\n",
    "submission = pd.read_csv(os.path.join(data_folder, \"sample_submission.csv\"))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入原始数据\n",
    "models = ['lr', 'lstm', 'svm', 'rf','lgb', 'textCNN', 'rank_averaged']\n",
    "train_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "for model in models:\n",
    "    # train_preds[model] = pd.read_csv(os.path.join(data_folder, \"%s_train_preds.csv\" % model))\n",
    "    test_preds[model] = pd.read_csv(os.path.join(data_folder, \"%s_submission.csv\" % model))\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class class_data():\n",
    "    \"\"\"按类别组织数据\"\"\"\n",
    "    def __init__(self, train, target=None):\n",
    "        self.__X = train\n",
    "        self.__models = list(self.__X.keys())\n",
    "        self.__class = list(self.__X[self.__models[0]].columns)\n",
    "        \n",
    "        if target is not None:\n",
    "            self.__y = target\n",
    "        else:\n",
    "            self.__y = None\n",
    "            \n",
    "        self.classes = self.__class\n",
    "        self.models = self.__models\n",
    "    \n",
    "    \n",
    "        \n",
    "    def __merge(self):\n",
    "        data = {}\n",
    "        for label in self.__class:\n",
    "            label_data = np.zeros((self.__X[self.__models[0]].shape[0], len(self.__models)))\n",
    "            label_data = pd.DataFrame(label_data); label_data.columns = self.__models\n",
    "            for model in self.__models:\n",
    "                label_data[model] = self.__X[model][label]\n",
    "            if self.__y is not None:\n",
    "                label_value = self.__y[label]\n",
    "                label_result = {'X': label_data, 'y': label_value}\n",
    "            else:\n",
    "                label_result = {'X': label_data}\n",
    "                \n",
    "            data[label] = label_result\n",
    "        return(data)\n",
    "    \n",
    "    @property\n",
    "    def data(self):\n",
    "        return(self.__merge())\n",
    "    \n",
    "    def __scoring(self, class_X, class_y):\n",
    "        roc_score = []\n",
    "        for model in self.__models:\n",
    "            roc_score.append(roc_auc_score(class_y, class_X[model]))\n",
    "        roc_score = pd.DataFrame(roc_score)\n",
    "        roc_score = roc_score.T; roc_score.columns = models\n",
    "        return(roc_score)\n",
    "        \n",
    "    @property\n",
    "    def roc_score(self):\n",
    "        data = self.data\n",
    "        score = {}\n",
    "        for label in self.__class:\n",
    "            score[label] = self.__scoring(data[label]['X'], data[label]['y'])\n",
    "        return (score)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-89-bef54376aad5>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-89-bef54376aad5>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    if flag > 0.5*lengtest_preds['lgb']th:\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def select_score(array):\n",
    "    data = array\n",
    "    data_min = min(data)\n",
    "    data_max = max(data)\n",
    "    length = len(array)\n",
    "    \n",
    "    flag = (data < 0.5).sum()\n",
    "    \n",
    "    if flag > 0.5*lengtest_preds['lgb']th:\n",
    "        score = data_min\n",
    "    elif flag < 0.5*length:\n",
    "        score = data_max\n",
    "    else:\n",
    "        score = data.mean()\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = class_data(train_preds, train_target).data\n",
    "#train_roc_score = class_data(train_preds, train_target).roc_score\n",
    "test_data = class_data(test_preds).data\n",
    "del test_data['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base ensembling: average\n",
    "for label in class_names:\n",
    "    X_test = test_data[label][\"X\"]\n",
    "    result = X_test.apply(np.mean, axis=1)\n",
    "    submission[label] = result\n",
    "    \n",
    "submission.to_csv(os.path.join(data_folder, \"ensemble_average_other_submission.csv\"), index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_matrix = pd.DataFrame(np.arange(42).reshape((6,7)))\n",
    "roc_matrix.index = class_names\n",
    "roc_matrix.columns = models + ['select', 'ave']\n",
    "# roc_matrix = roc_matrix.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in ['toxic']:\n",
    "# for label in class_names:\n",
    "    X_train = train_data[label]['X']\n",
    "    y_train = train_data[label]['y']\n",
    "    roc_score = train_roc_score[label]\n",
    "    #select = X_train.apply(select_score, axis=1)\n",
    "    #ave = X_train.apply(np.mean, axis=1)\n",
    "    \n",
    "    #roc_score['select'] = roc_auc_score(y_train, select)\n",
    "    #roc_score['ave'] = roc_auc_score(y_train, ave)\n",
    "    \n",
    "    #roc_matrix.loc[label,] = roc_score.iloc[0,].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "outFilename = os.path.join(data_folder, \"ensemble_lr_submission.csv\")\n",
    "for label in class_names:\n",
    "    X_train = train_data[label]['X']\n",
    "    y_train = train_data[label]['y']\n",
    "    X_test = test_data[label]['X']\n",
    "                           \n",
    "    clf.fit(X_train, y_train)\n",
    "    test_preds = clf.predict_proba(X_test)[:,1]\n",
    "                           \n",
    "    submission[label] = test_preds\n",
    "\n",
    "submission.to_csv(os.path.join(data_folder, \"ensemble_lr_submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "[0.99985959 0.99983043 0.99981156]\n"
     ]
    }
   ],
   "source": [
    "# roc_scorer = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "pars = {\n",
    "    # \"max_features\": [50, 200],\n",
    "    \"n_estimators\": [20,100],\n",
    "    # \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"min_samples_leaf\": [2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "clf = LogisticRegression()\n",
    "roc_score = cross_val_score(clf, X_train, y_train, scoring='roc_auc')\n",
    "# grid = GridSearchCV(clf, pars, scoring='roc_auc', n_jobs=1, cv=5)\n",
    "# grid.fit(train_tfidf, train_df[\"toxic\"],)\n",
    "# print(\"Accuracy: {0:.4f}%\".format(grid.best_score_ * 100))\n",
    "gc.collect()\n",
    "print(clf)\n",
    "print(roc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
