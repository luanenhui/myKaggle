{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luan/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "import os,gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_folder = os.path.expanduser(\"~\")\n",
    "data_folder = os.path.join(os.path.expanduser(\"~\"), 'E:/git/database/Toxic_Comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_target = pd.read_csv(os.path.join(data_folder, \"train.csv\"))\n",
    "train_target = train_target[class_names]\n",
    "submission = pd.read_csv(os.path.join(data_folder, \"sample_submission.csv\"))\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = ['lr', 'lstm', 'svm', 'rf','lgb']\n",
    "# models = ['lr', 'lstm', 'svm', ‘rf', 'lgb']\n",
    "\n",
    "train_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "for model in models:\n",
    "    train_preds[model] = pd.read_csv(os.path.join(data_folder, \"%s_train_preds.csv\" % model))\n",
    "    test_preds[model] = pd.read_csv(os.path.join(data_folder, \"%s_submission.csv\" % model))\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_data = {}\n",
    "for i in class_names:\n",
    "    data = np.zeros((train_target.shape[0], len(models)))\n",
    "    data = pd.DataFrame(data); data.columns = models\n",
    "    for model in models:\n",
    "        data[model] = train_preds[model][i]\n",
    "    train_class_data[i] = data\n",
    "    del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class_data = {}\n",
    "for i in class_names:\n",
    "    data = np.zeros((submission.shape[0], len(models)))\n",
    "    data = pd.DataFrame(data); data.columns = models\n",
    "    for model in models:\n",
    "        data[model] = test_preds[model][i]\n",
    "    test_class_data[i] = data\n",
    "    del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(y_true, y_score, models):\n",
    "    score = np.zeros((len(models),))\n",
    "    idx = 0\n",
    "    for model in models:\n",
    "        score[idx] = roc_auc_score(y_true, y_score[model])\n",
    "        idx += 1\n",
    "    # score = score.mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base ensembling: select extreme\n",
    "def select_score(array):\n",
    "    data = array\n",
    "    data_min = min(data)\n",
    "    data_max = max(data)\n",
    "    length = len(array)\n",
    "    \n",
    "    flag = (data < 0.5).sum()\n",
    "    \n",
    "    if flag > 0.5*length:\n",
    "        score = data_min\n",
    "    elif flag < 0.5*length:\n",
    "        score = data_max\n",
    "    else:\n",
    "        score = data.mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "for label in class_names:\n",
    "    \n",
    "    X_test = test_class_data[label]\n",
    "    result = X_test.apply(select_score, axis=1)\n",
    "    submission[label] = result\n",
    "\n",
    "submission.to_csv(os.path.join(data_folder, \"ensemble_select2_submission.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base ensembling: average\n",
    "for label in class_names:\n",
    "    X_test = test_class_data[label]\n",
    "    result = X_test.apply(np.mean, axis=1)\n",
    "    submission[label] = result\n",
    "    \n",
    "submission.to_csv(os.path.join(data_folder, \"ensemble_average_submission.csv\"), index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_submission_select = deepcopy(train_preds['lgb'])\n",
    "for label in class_names:\n",
    "    X_train = train_class_data[label]\n",
    "    result = X_train.apply(select_score, axis=1)\n",
    "    train_submission_select[label] = result\n",
    "\n",
    "train_submission_ave = deepcopy(train_preds['lgb'])\n",
    "for label in class_names:\n",
    "    X_train = train_class_data[label]\n",
    "    result = X_train.apply(np.mean, axis=1)\n",
    "    train_submission_ave[label] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.98599063, 0.98852906, 0.9666938 , 0.99991472, 0.99132291]),\n",
       " 0.9954486343493429,\n",
       " 0.9973911182788009]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = scoring(train_target['toxic'], train_class_data['toxic'], models)\n",
    "select = roc_auc_score(train_target['toxic'], train_submission_select['toxic'])\n",
    "ave = roc_auc_score(train_target['toxic'], train_submission_ave['toxic'])\n",
    "[lr, select, ave]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ensembler\n",
    "ensemble_logistic_submission: LogisticRegression(), ['lr‘，'lgb', 'lstm', 'rf', 'svm']\n",
    "<br>\n",
    "ensemble_logistic_submission: LogisticRegression(), ['lr‘，'lgb', 'lstm', 'svm']\n",
    "<br>\n",
    "ensemble_select_submission: select_score(), ['lr‘，'lgb', 'lstm', 'rf', 'svm']\n",
    "<br>\n",
    "ensemble_select2_submission: select_score(), ['lr‘，'lgb', 'lstm', 'svm']\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9859906303243691, 0.9893216925063395]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = roc_auc_score(train_target['toxic'],train_preds['lr']['toxic'])\n",
    "select = roc_auc_score(train_target['toxic'], train_submission['toxic'])\n",
    "[lr, select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
